{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a751c3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATABASE = \"\"\n",
    "DB_USER = \"\"\n",
    "DB_HOST = \"\"\n",
    "DB_PASSWORD = \"\"\n",
    "DB_PORT = \"\"\n",
    "\n",
    "JSON_QUESTIONS = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ee6066",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "def connect_postgresql():\n",
    "        # Open database connection\n",
    "        # Connect to the database\n",
    "        db = psycopg2.connect(\n",
    "            f\"dbname={DATABASE} user={DB_USER} host={DB_HOST} password={DB_PASSWORD} port={DB_PORT}\"\n",
    "        )\n",
    "        return db\n",
    "\n",
    "def execute_postgresql_query(cursor, query):\n",
    "    cursor.execute(query)\n",
    "    result = cursor.fetchall()\n",
    "    return result\n",
    "\n",
    "def perform_query_on_postgresql_databases(query):\n",
    "    db = connect_postgresql()\n",
    "    cursor = db.cursor()\n",
    "    result = execute_postgresql_query(cursor, query)\n",
    "    db.close()\n",
    "    return result\n",
    "\n",
    "# Excecute a sql query for a postgre database. Returns answer truncated for reponses of more than a characters, to prevent passing answers longer than model context window.\n",
    "def execute_sql_query(sql_query: str):\n",
    "    MAX_LENGTH = 1000  \n",
    "    a = perform_query_on_postgresql_databases(sql_query)\n",
    "    answer_str = str(a)\n",
    "    return answer_str[:MAX_LENGTH] + (\"...\" if len(answer_str) > MAX_LENGTH else \"\")\n",
    "\n",
    "EXECUTE_INTERMEDIATE_SQL_TOOL_SCHEMA = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"execute_sql_query\",\n",
    "        \"description\": \"Executes a given SQL SELECT query against the database. Use this to test parts of a query, explore data patterns, or verify assumptions for constructing the final SQL. The query should be specific and aim to return a small result set (e.g., use LIMIT). Only SELECT queries are permitted.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"sql_query\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The PostgreSQL SELECT query to execute for analysis or testing.\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"sql_query\"]\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1d6e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = {}\n",
    "#Telling the models that they have lower count of max auto replies, since they do not have accurate tracking of which response they are in and this helps avoind mistakes where the model thinks it has 'time' left.\n",
    "prompts[\"SQL_REPAIR\"] = \"\"\"\n",
    "You are an expert SQL engineer. Your task is to complete a partially correct, incomplete, or non-working SQL query so that it answers the original user question.\n",
    "You should aim to finish it as fast as possible as long as it works.\n",
    "To asses wether it works or not you will be given access to a tool to run SQL.\n",
    "\n",
    "You will be given:\n",
    "- The chat history with the other agent. In this chat history there will be:\n",
    "    - The schema of the database.\n",
    "    - The question to be answered, with evidence.\n",
    "    - The SQL process to generate an SQL by another agent.\n",
    "- Any error messages or execution feedback from running the query\n",
    "\n",
    "Your job:\n",
    "1. Analyze the provided SQL query, error messages, and context.\n",
    "2. Identify and fix any syntax errors, logical mistakes, or missing parts.\n",
    "3. Ensure the repaired SQL query answers the original question as intended.\n",
    "4. Use the schema and context to ensure all table and column names are correct.\n",
    "5. Return ONLY the repaired SQL query.\n",
    "\n",
    "You will have a max of 4 auto replies.\n",
    "Remember to try to get it working in as few steps as possibe and use the tool to test it. Terminate only if it works.\n",
    "\n",
    "- Your FINAL response MUST be ONLY the SQL query to answer the original NLQ. Do not include explanations or markdown.\n",
    "- You then terminate by puting \"TERMINATE_CHAT\" in that same message.\n",
    "\"\"\"\n",
    "prompts[\"SQL_GENERATOR_SYSTEM_MESSAGE\"] = \"\"\"You are an expert SQL Reasoner and Generator.\n",
    "Your task is to generate a final, syntactically correct, and semantically accurate PostgreSQL query\n",
    "to answer the given Natural Language Question (NLQ) based on the provided context (schema, evidence, data profile, cleaning advice).\n",
    "\n",
    "You have a tool to help you understand the schema and data before committing to a final query. As well as run tests:\n",
    "1.  `execute_sql_query(sql_query)`: To run a SELECT query against the database to test a hypothesis, check data values, or explore relationships. Use this iteratively if needed. For example, you can use it to check distinct values in a column before deciding on a filter, or to verify a join condition on a small sample. Ensure queries are specific and return small results (e.g., use LIMIT).\n",
    "\n",
    "Reasoning Process:\n",
    "1. Understand the NLQ and the initially provided schema and context.\n",
    "2. If you need to understand actual data patterns, NULL distributions, or test a query fragment, use the `execute_intermediate_sql_query` tool. You can use this tool multiple times if necessary to refine your understanding or query.\n",
    "3. Based on all information gathered, construct your final PostgreSQL query.\n",
    "4. Try the query and iterated on it when appropiate. \n",
    "\n",
    "When you have an idea for the final answer and see if its responding appropiately.\n",
    "Once you produce a final SQL query or an answer, do not repeat it or continue generating more.\n",
    "You will have a max of 13 auto replies.\n",
    "Make sure the SQL answer works.\n",
    "Do not terminate unless you reach the max or get a working SQL.\n",
    "\n",
    "- Your FINAL response MUST be ONLY the SQL query to answer the original NLQ. Do not include explanations or markdown.\n",
    "- You then terminate by puting \"TERMINATE_CHAT\" in that same message.\n",
    "\"\"\"\n",
    "prompts[\"ANSWER_DECIDER\"] = \"\"\"You are an expert SQL evaluator.\n",
    "You will be given a question, evidence (extra context) and two answers, the first the ground truth and then the best guess.\n",
    "The answers will be preluded with their corresponding SQL.\n",
    "You need to determine if the guess is appropiate with the context of the correct answer and question.\n",
    "It will be considered correct if it holds the same information, even with different formats.\n",
    "Example:\n",
    "    Question: In what month of 2025 did we achieve X?\n",
    "    Correct: March\n",
    "    Guess: 03/25\n",
    "It will also be considered correct if it has extra data. \n",
    "Example:\n",
    "    Question: What month of 2025 had the highest income?\n",
    "    Correct: March\n",
    "    Guess: [03, $542]\n",
    "It will not be considered correct if numeric values such as floats, decimals and integers have discrepancies.\n",
    "Example:\n",
    "    Question: How much were earnings in July 2022?\n",
    "    Correct: 486.64\n",
    "    Guess: 480\n",
    "Answer with only CORRECT or INCORRECT.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796a9c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen import UserProxyAgent, ConversableAgent\n",
    "import os\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "import pickle\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d021f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"model\" = \"gpt-4.1-nano\"\n",
    "# \"model\":\"gpt-4.1\",\n",
    "\n",
    "LLM_CONFIG_LIST = [\n",
    "    {\n",
    "        \"model\":\"o4-mini\",\n",
    "        \"api_key\": os.getenv(\"OPENAI_API_KEY\")\n",
    "    }\n",
    "]\n",
    "\n",
    "sql_generator_agents = {}\n",
    "llm_config = {\"config_list\": LLM_CONFIG_LIST, \"cache_seed\": 42, \"timeout\": 180}\n",
    "agent_specific_llm_config = {\n",
    "    **llm_config,\n",
    "    \"tools\": [\n",
    "        EXECUTE_INTERMEDIATE_SQL_TOOL_SCHEMA \n",
    "    ],\n",
    "}\n",
    "SQLGenerator = ConversableAgent(\n",
    "    name=\"SQLGenerator\",\n",
    "    system_message=prompts[\"SQL_GENERATOR_SYSTEM_MESSAGE\"],\n",
    "    llm_config=agent_specific_llm_config,\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=15,\n",
    "    code_execution_config=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a38b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM__REPAIR_CONFIG_LIST = [\n",
    "    {\n",
    "        \"model\":\"gpt-4.1\",\n",
    "        \"api_key\":os.getenv(\"OPENAI_API_KEY\")\n",
    "    }\n",
    "]\n",
    "llm_config = {\"config_list\": LLM__REPAIR_CONFIG_LIST, \"cache_seed\": 42, \"timeout\": 180}\n",
    "agent_specific_llm_config = {\n",
    "    **llm_config,\n",
    "    \"tools\": [\n",
    "        EXECUTE_INTERMEDIATE_SQL_TOOL_SCHEMA\n",
    "    ],\n",
    "}\n",
    "SQLRepair = ConversableAgent(\n",
    "    name=\"SQLRepair\",\n",
    "    system_message=prompts[\"SQL_REPAIR\"], \n",
    "    llm_config=agent_specific_llm_config,\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=5,\n",
    "    code_execution_config=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1c2b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_proxy = UserProxyAgent(\n",
    "    name=\"UserProxyOrchestrator\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    code_execution_config=False,\n",
    "    function_map={\n",
    "        \"execute_sql_query\": execute_sql_query\n",
    "    },\n",
    "    is_termination_msg=lambda msg: isinstance(msg.get(\"content\", \"\"), str) and \"terminate_chat\" in msg[\"content\"].lower()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4f7ff5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Function to get questions to be tested, with specific distributions if desired for testing. \n",
    "def load_and_sample_by_difficulty(json_path: str, max_simple: int, max_moderate: int, max_challenging: int) -> List[Dict[str, Any]]:\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        all_qs = json.load(f)\n",
    "    buckets = {\n",
    "        \"simple\": [],\n",
    "        \"moderate\": [],\n",
    "        \"challenging\": []\n",
    "    }\n",
    "    limits = {\n",
    "        \"simple\": max_simple,\n",
    "        \"moderate\": max_moderate,\n",
    "        \"challenging\": max_challenging\n",
    "    }\n",
    "    for q in all_qs:\n",
    "        diff = q.get(\"difficulty\", \"\")\n",
    "        if diff in buckets and len(buckets[diff]) < limits[diff]:\n",
    "            buckets[diff].append(q)\n",
    "        if all(len(buckets[d]) >= limits[d] for d in buckets):\n",
    "            break\n",
    "    sampled = buckets[\"simple\"] + buckets[\"moderate\"] + buckets[\"challenging\"]\n",
    "    return sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77be9145",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "queries = load_and_sample_by_difficulty( json_path=JSON_QUESTIONS, max_simple=500, max_moderate=500, max_challenging=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4090c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_full_schema_ddl() -> str:\n",
    "    \"\"\"\n",
    "    Fetches CREATE TABLE DDL statements for tables in the 'public' schema of a PostgreSQL database.\n",
    "    Includes columns, data types, NULL constraints, defaults, primary keys, foreign keys, and basic CHECK constraints.\n",
    "    \"\"\"\n",
    "    conn = connect_postgresql()\n",
    "    if not conn:\n",
    "        return \"-- ERROR: Could not establish database connection to fetch schema.\"\n",
    "\n",
    "    schema_ddl_parts = []\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            query_tables = \"\"\"\n",
    "                SELECT tablename\n",
    "                FROM pg_catalog.pg_tables\n",
    "                WHERE schemaname = 'public'\n",
    "                ORDER BY tablename;\n",
    "            \"\"\"\n",
    "            cur.execute(query_tables)\n",
    "            all_table_names = [row[0] for row in cur.fetchall()]\n",
    "            tables_to_process = all_table_names\n",
    "            \n",
    "            if not tables_to_process:\n",
    "                return \"-- No tables found or selected in 'public' schema to generate DDL.\"\n",
    "\n",
    "            for table_name in tables_to_process:\n",
    "                table_definition_parts = []\n",
    "\n",
    "                query_columns = \"\"\"\n",
    "                    SELECT\n",
    "                        column_name,\n",
    "                        data_type,\n",
    "                        udt_name, \n",
    "                        is_nullable,\n",
    "                        column_default,\n",
    "                        character_maximum_length,\n",
    "                        numeric_precision,\n",
    "                        numeric_scale,\n",
    "                        datetime_precision\n",
    "                    FROM information_schema.columns\n",
    "                    WHERE table_schema = 'public' AND table_name = %s\n",
    "                    ORDER BY ordinal_position;\n",
    "                \"\"\"\n",
    "                cur.execute(query_columns, (table_name,))\n",
    "                columns_data = cur.fetchall()\n",
    "\n",
    "                column_defs = []\n",
    "                for col_name, data_type, udt_name, is_nullable, col_default, char_max_len, num_prec, num_scale, dt_prec in columns_data:\n",
    "                    col_type_str = udt_name \n",
    "                    if data_type == 'ARRAY':\n",
    "                        elem_type_query = \"\"\"\n",
    "                            SELECT e.typname\n",
    "                            FROM pg_type t\n",
    "                            JOIN pg_namespace n ON n.oid = t.typnamespace\n",
    "                            JOIN pg_type e ON e.oid = t.typelem\n",
    "                            WHERE n.nspname = 'public' AND t.typname = %s;\n",
    "                        \"\"\"\n",
    "                        cur.execute(elem_type_query, (udt_name,))\n",
    "                        elem_res = cur.fetchone()\n",
    "                        if elem_res:\n",
    "                            col_type_str = f\"{elem_res[0].upper()}[]\"\n",
    "                        else:\n",
    "                            base_type_from_udt = udt_name[1:] if udt_name.startswith('_') else udt_name\n",
    "                            col_type_str = f\"{base_type_from_udt.upper()}[]\"\n",
    "                    elif data_type.lower() in ['character varying', 'varchar']:\n",
    "                        col_type_str = f\"VARCHAR({char_max_len})\" if char_max_len else \"VARCHAR\"\n",
    "                    elif data_type.lower() in ['character', 'char']:\n",
    "                        col_type_str = f\"CHAR({char_max_len})\" if char_max_len else \"CHAR\"\n",
    "                    elif data_type.lower() == 'numeric':\n",
    "                        if num_prec and num_scale is not None:\n",
    "                            col_type_str = f\"NUMERIC({num_prec}, {num_scale})\"\n",
    "                        elif num_prec:\n",
    "                            col_type_str = f\"NUMERIC({num_prec})\"\n",
    "                        else:\n",
    "                            col_type_str = \"NUMERIC\"\n",
    "                    elif data_type.lower().startswith('timestamp'):\n",
    "                        col_type_str = f\"TIMESTAMP({dt_prec})\" if dt_prec is not None else data_type.upper()\n",
    "                    elif data_type.lower().startswith('time'):\n",
    "                        col_type_str = f\"TIME({dt_prec})\" if dt_prec is not None else data_type.upper()\n",
    "                    else:\n",
    "                        col_type_str = data_type.upper()\n",
    "\n",
    "                    col_def = f\"    \\\"{col_name}\\\" {col_type_str}\"\n",
    "                    if is_nullable == 'NO':\n",
    "                        col_def += \" NOT NULL\"\n",
    "                    if col_default:\n",
    "                        col_def += f\" DEFAULT {col_default}\"\n",
    "                    column_defs.append(col_def)\n",
    "                \n",
    "                table_definition_parts.extend(column_defs)\n",
    "\n",
    "                query_pk = \"\"\"\n",
    "                    SELECT kcu.column_name\n",
    "                    FROM information_schema.table_constraints AS tc\n",
    "                    JOIN information_schema.key_column_usage AS kcu\n",
    "                        ON tc.constraint_name = kcu.constraint_name AND tc.table_schema = kcu.table_schema\n",
    "                    WHERE tc.constraint_type = 'PRIMARY KEY'\n",
    "                        AND tc.table_name = %s AND tc.table_schema = 'public';\n",
    "                \"\"\"\n",
    "                cur.execute(query_pk, (table_name,))\n",
    "                pk_columns = [f\"\\\"{row[0]}\\\"\" for row in cur.fetchall()]\n",
    "                if pk_columns:\n",
    "                    table_definition_parts.append(f\"    PRIMARY KEY ({', '.join(pk_columns)})\")\n",
    "\n",
    "                query_fk = \"\"\"\n",
    "                    SELECT\n",
    "                        tc.constraint_name,\n",
    "                        kcu.column_name AS local_column,\n",
    "                        ccu.table_name AS foreign_table_name,\n",
    "                        ccu.column_name AS foreign_column_name\n",
    "                    FROM information_schema.table_constraints AS tc\n",
    "                    JOIN information_schema.key_column_usage AS kcu\n",
    "                        ON tc.constraint_name = kcu.constraint_name AND tc.table_schema = kcu.table_schema\n",
    "                    JOIN information_schema.constraint_column_usage AS ccu\n",
    "                        ON tc.constraint_name = ccu.constraint_name AND tc.table_schema = ccu.table_schema\n",
    "                    WHERE tc.constraint_type = 'FOREIGN KEY'\n",
    "                        AND tc.table_name = %s AND tc.table_schema = 'public';\n",
    "                \"\"\"\n",
    "                cur.execute(query_fk, (table_name,))\n",
    "                fks_data = cur.fetchall()\n",
    "                fk_constraints_grouped = {}\n",
    "                for cons_name, loc_col, f_table, f_col in fks_data:\n",
    "                    if cons_name not in fk_constraints_grouped:\n",
    "                        fk_constraints_grouped[cons_name] = {\n",
    "                            'local_columns': [],\n",
    "                            'foreign_table_name': f_table,\n",
    "                            'foreign_columns': []\n",
    "                        }\n",
    "                    fk_constraints_grouped[cons_name]['local_columns'].append(f\"\\\"{loc_col}\\\"\")\n",
    "                    fk_constraints_grouped[cons_name]['foreign_columns'].append(f\"\\\"{f_col}\\\"\")\n",
    "                \n",
    "                for cons_name, fk_info in fk_constraints_grouped.items():\n",
    "                    local_cols_str = \", \".join(sorted(list(set(fk_info['local_columns']))))\n",
    "                    foreign_cols_str = \", \".join(sorted(list(set(fk_info['foreign_columns']))))\n",
    "                    fk_def = (f\"    CONSTRAINT \\\"{cons_name}\\\" FOREIGN KEY ({local_cols_str}) \"\n",
    "                                f\"REFERENCES public.\\\"{fk_info['foreign_table_name']}\\\" ({foreign_cols_str})\")\n",
    "                    table_definition_parts.append(fk_def)\n",
    "\n",
    "                query_check = \"\"\"\n",
    "                    SELECT cc.constraint_name, cc.check_clause \n",
    "                    FROM information_schema.check_constraints cc\n",
    "                    JOIN information_schema.table_constraints tc\n",
    "                        ON cc.constraint_name = tc.constraint_name AND cc.constraint_schema = tc.table_schema\n",
    "                    WHERE tc.table_name = %s AND tc.table_schema = 'public' AND tc.constraint_type = 'CHECK';\n",
    "                \"\"\"\n",
    "                cur.execute(query_check, (table_name,))\n",
    "                check_constraints = cur.fetchall()\n",
    "                for cons_name, check_clause in check_constraints:\n",
    "                    table_definition_parts.append(f\"    CONSTRAINT \\\"{cons_name}\\\" CHECK ({check_clause})\")\n",
    "                \n",
    "                if table_definition_parts:\n",
    "                    schema_ddl_parts.append(\n",
    "                        f\"CREATE TABLE public.\\\"{table_name}\\\" (\\n\" + \n",
    "                        \",\\n\".join(table_definition_parts) + \n",
    "                        \"\\n);\"\n",
    "                    )\n",
    "\n",
    "        conn.rollback() \n",
    "        return \"\\n\\n\".join(schema_ddl_parts) if schema_ddl_parts else \"-- No DDL generated (no tables or error).\"\n",
    "    except Exception as e:\n",
    "        if conn and not conn.closed : conn.rollback()\n",
    "        return f\"-- Unexpected error fetching schema: {e}\"\n",
    "    finally:\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2171782b",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_sql = fetch_full_schema_ddl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3a8471-9a2e-46e0-932c-62964ffde570",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "LLM_DECIDER_CONFIG_LIST = [\n",
    "    {\n",
    "        \"model\":\"gpt-4.1-nano\",\n",
    "        \"api_key\": os.getenv(\"OPENAI_API_KEY\") # Or environment variable\n",
    "    }\n",
    "]\n",
    "\n",
    "llm_config = {\"config_list\": LLM_DECIDER_CONFIG_LIST, \"cache_seed\": 42, \"timeout\": 180}\n",
    "agent_specific_llm_config = {\n",
    "    **llm_config,\n",
    "}\n",
    "answer_evaluator = ConversableAgent(\n",
    "        name=\"answer_evaluator\",\n",
    "        system_message=prompts[\"ANSWER_DECIDER\"], # Prompt needs to mention these new tools\n",
    "        llm_config=agent_specific_llm_config,\n",
    "        human_input_mode=\"NEVER\",\n",
    "        max_consecutive_auto_reply=1, # Might need more turns for multiple tool calls\n",
    "        code_execution_config=False,\n",
    "        # is_termination_msg=lambda msg: \"TERMINATE_CHAT\" in msg[\"content\"],\n",
    "    )\n",
    "\n",
    "user_proxy_decider = UserProxyAgent(\n",
    "    name=\"UserProxyOrchestrator\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    code_execution_config=False,\n",
    "    function_map={\n",
    "        \"execute_sql_query\": execute_sql_query\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71226f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results=[]\n",
    "def generate_answer(item):\n",
    "    qid = item['question_id']\n",
    "    Q = item['question']\n",
    "    E = item['evidence']\n",
    "    S = item['SQL']\n",
    "    C = schema_sql\n",
    "    sql_guess = \"\"\n",
    "    answer =  \"\"\n",
    "\n",
    "    target = execute_sql_query(S)\n",
    "    \n",
    "    try:\n",
    "        chat_result = user_proxy.initiate_chat(\n",
    "            recipient=SQLGenerator,\n",
    "            message=f\"\\nQuestion:\\n{Q}nEvidence:\\n{E}nSCHEMA:\\n{C}\",\n",
    "            clear_history=True,\n",
    "            silent=False,\n",
    "        )\n",
    "        last_msg = chat_result.chat_history[-1]\n",
    "        sql_guess = last_msg[\"content\"].replace(\"TERMINATE_CHAT\", \"\").strip()\n",
    "        try:\n",
    "            answer = execute_sql_query(sql_guess)\n",
    "        except Exception as e:\n",
    "            chat_repair = user_proxy.initiate_chat(\n",
    "                recipient=SQLRepair,\n",
    "                message=str(chat_result.chat_history),\n",
    "                clear_history=True,\n",
    "                silent=False,\n",
    "            )\n",
    "            answer = f\"{e}\"\n",
    "            last_msg = chat_repair.chat_history[-1]\n",
    "            sql_guess = last_msg[\"content\"].replace(\"TERMINATE_CHAT\", \"\").strip()\n",
    "            chat_result = {\"chat_result\":chat_result, \"chat_repair\":chat_repair}\n",
    "            try:\n",
    "                answer = execute_sql_query(sql_guess)\n",
    "            except Exception as e:\n",
    "                answer = f\"{e}\"\n",
    "    except Exception as e:\n",
    "        chat_result = f\"{e}\"\n",
    "    try:\n",
    "        chat_decider = user_proxy_decider.initiate_chat(\n",
    "            recipient=answer_evaluator,\n",
    "            message=f\"\\nQuestion:\\n{Q}\\nEvidence:\\n{E}\\nSQL of correct:\\n{S}\\nCorrect:\\n{target}\\nSQL of guess:\\n{sql_guess}\\nGuess:\\n{answer}\",\n",
    "            clear_history=True,\n",
    "            silent=False,\n",
    "                # human_input_mode=\"NEVER\",  # Never ask for human input.\n",
    "        )\n",
    "        chat_decider = chat_decider.chat_history[1][\"content\"]\n",
    "    except Exception as e:\n",
    "        chat_decider = f\"{e}\"\n",
    "    return {\n",
    "        \"id\":qid,\n",
    "        \"target\":target,\n",
    "        \"target_sql\":S,\n",
    "        \"answer\":answer,\n",
    "        \"answer_sql\":sql_guess,\n",
    "        \"full_answer\":chat_result,\n",
    "        \"evaluation\": chat_decider\n",
    "    }\n",
    "\n",
    "try:\n",
    "    for item in queries:\n",
    "        result = generate_answer(item)\n",
    "        results.append(result)  \n",
    "finally:\n",
    "    with open(\"results.pkl\", \"wb\") as f:\n",
    "        pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ce056e-51e6-4064-b6de-d9af5292289e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def print_wrapped_table(columns, headers, col_widths=None, add_row_numbers=True):\n",
    "    num_cols = len(headers)\n",
    "\n",
    "    # Validate data\n",
    "    if len(columns) != num_cols:\n",
    "        raise ValueError(\"Number of column data lists must match number of headers\")\n",
    "\n",
    "    if col_widths is None:\n",
    "        col_widths = [30] * num_cols\n",
    "    elif len(col_widths) < num_cols:\n",
    "        col_widths += [30] * (num_cols - len(col_widths))\n",
    "\n",
    "    # Add row number column\n",
    "    rows = list(zip(*columns))  # Transpose to rows\n",
    "    row_numbers = [str(i + 1) for i in range(len(rows))]\n",
    "\n",
    "    if add_row_numbers:\n",
    "        headers = ['#'] + headers\n",
    "        col_widths = [4] + col_widths\n",
    "        columns = [row_numbers] + columns\n",
    "        rows = list(zip(*columns))\n",
    "\n",
    "    # Print headers\n",
    "    header_line = \"  \".join(f\"{h:<{w}}\" for h, w in zip(headers, col_widths))\n",
    "    print(header_line)\n",
    "    print(\"-\" * len(header_line))\n",
    "\n",
    "    for row in rows:\n",
    "        # Wrap each cell\n",
    "        wrapped_cells = [\n",
    "            textwrap.wrap(str(cell), width) or ['']\n",
    "            for cell, width in zip(row, col_widths)\n",
    "        ]\n",
    "\n",
    "        max_lines = max(len(cell) for cell in wrapped_cells)\n",
    "\n",
    "        # Pad wrapped cells\n",
    "        for cell in wrapped_cells:\n",
    "            cell += [''] * (max_lines - len(cell))\n",
    "\n",
    "        # Print row line by line\n",
    "        for i in range(max_lines):\n",
    "            line = \"  \".join(f\"{wrapped_cells[j][i]:<{col_widths[j]}}\" for j in range(len(headers)))\n",
    "            print(line)\n",
    "\n",
    "        print()  # Space between rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dbe8e7-e3e7-472b-8573-f8dd5df2f5a2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count_statuses(strings):\n",
    "    counts = {\"CORRECT\": 0, \"INCORRECT\": 0, \"UNKNOWN\": 0}\n",
    "    \n",
    "    for s in strings:\n",
    "        s_clean = s.strip().upper()\n",
    "        if s_clean == \"CORRECT\":\n",
    "            counts[\"CORRECT\"] += 1\n",
    "        elif s_clean == \"INCORRECT\":\n",
    "            counts[\"INCORRECT\"] += 1\n",
    "        else:\n",
    "            counts[\"UNKNOWN\"] += 1\n",
    "\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7a3135-7761-403e-b6f4-886f9103cb2b",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(count_statuses([result[\"evaluation\"] for result in results]))\n",
    "print_wrapped_table(\n",
    "    columns = [\n",
    "        [result[\"id\"] for result in results],\n",
    "        [result[\"target_sql\"] for result in results],\n",
    "        [result[\"target\"] for result in results],\n",
    "        [result[\"answer_sql\"] for result in results],\n",
    "        [result[\"answer\"] for result in results],\n",
    "        [result[\"evaluation\"] for result in results]\n",
    "    ], \n",
    "    headers = [\"id\", \"target_sql\", \"target\", \"answer_sql\", \"answer\", \"evaluation\"], \n",
    "    col_widths = [5,40,10,40,10,10]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562ead63-f372-4f92-86dc-efd4d991c1a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_question_and_evidence(data, question_id):\n",
    "    for item in data:\n",
    "        if item.get(\"question_id\") == question_id:\n",
    "            return {\n",
    "                \"question\": item.get(\"question\"),\n",
    "                \"evidence\": item.get(\"evidence\")\n",
    "            }\n",
    "    # If not found, return empty strings to avoid unbound variable error\n",
    "    return {\n",
    "        \"question\": \"\",\n",
    "        \"evidence\": \"\"\n",
    "    }\n",
    "\n",
    "# Filter results to only those that are not correct\n",
    "print_wrapped_table(\n",
    "    columns = [\n",
    "        [result[\"id\"] for result in results],\n",
    "        [get_question_and_evidence(queries, result[\"id\"])[\"question\"] for result in results],\n",
    "        [get_question_and_evidence(queries, result[\"id\"])[\"evidence\"] for result in results],\n",
    "        [result[\"target_sql\"] for result in results],\n",
    "        [result[\"target\"] for result in results],\n",
    "        [result[\"answer_sql\"] for result in results],\n",
    "        [result[\"answer\"] for result in results],\n",
    "        [result[\"evaluation\"] for result in results]\n",
    "    ],\n",
    "    headers = [\"id\",\"question\", \"evidence\", \"target_sql\", \"target\", \"answer_sql\", \"answer\", \"evaluation\"],\n",
    "    col_widths = [5,30,30,30,10,30,10,10]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
